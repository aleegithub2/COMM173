{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb1c547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data: This section will describe my procedures for collecting my data.\n",
    "#These lines are designed to import both pandas and selenium in order to operate the scraper as well as store the data.\n",
    "#This code is adapted from Professor Jeong's code provided to us in class.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#This is the input for the URL, specifically that of \"climate\" videos, and only videos.\n",
    "#The following lines open the window in order to perform the scrape.\n",
    "url = 'https://www.youtube.com/results?search_query=climate&sp=EgIQAQ%253D%253D'\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "\n",
    "#This section, taken from Professor Jeong, is designed to scroll to the bottom of the page and collect the video data.\n",
    "def scroll_to_bottom(driver):\n",
    "    old_position = 0\n",
    "    new_position = None\n",
    "    \n",
    "    while (new_position != old_position):\n",
    "        old_position = driver.execute_script((\"return (window.pageYOffset !== undefined) ?\"\n",
    "                                            \" window.pageYOffset : (document.documentElement ||\"\n",
    "                                            \" document.body.parentNode || document.body);\"))\n",
    "        time.sleep(5)\n",
    "        driver.execute_script((\"var scrollingElement = (document.scrollingElement ||\"\n",
    "                              \" document.body);scrollingElement.scrollTop = \"\n",
    "                              \" scrollingElement.scrollHeight\"))\n",
    "        \n",
    "        time.sleep(5)\n",
    "        new_position = driver.execute_script((\"return (window.pageYOffset !== undefined) ?\"\n",
    "                                             \" window.pageYOffset : (document.documentElement ||\"\n",
    "                                             \" document.body.parentNode || documentBody);\"))        \n",
    "scroll_to_bottom(driver)\n",
    "\n",
    "#This following segment, adapted from Professor Jeong, will tally up the links of the various videos.\n",
    "user_data = driver.find_elements(by=By.XPATH,value='//*[@id=\"video-title\"]')\n",
    "links = []\n",
    "for i in user_data:\n",
    "    if (i.get_attribute('href') != None):\n",
    "        links.append(i.get_attribute('href'))\n",
    "#Next, a dataframe is created to store the link, title, view count, like count, and dislike count.\n",
    "\n",
    "df = pd.DataFrame(columns = ['link', 'title', 'views', 'likes', 'dislikes'])\n",
    "\n",
    "#This next segment uses the links in order to extract the remainder of the data.\n",
    "\n",
    "for x in links:\n",
    "    driver.get(x)\n",
    "    v_id = x\n",
    "    v_title = wait.until(EC.presence_of_element_located(\n",
    "                   (By.CSS_SELECTOR,\"h1.style-scope.ytd-watch-metadata yt-formatted-string\"))).text\n",
    "    view_count = driver.find_element_by_css_selector(\".view-count\").text\n",
    "    like_count = driver.find_element_by_css_selector(\".like-button-renderer-like-button span\").text\n",
    "    dislike_count = driver.find_element_by_css_selector(\".like-button-renderer-dislike-button span\").text\n",
    "\n",
    "#This next line adds all the variables to a dataframe.\n",
    "    df.loc[len(df)] = [v_id, v_title, view_count, like_count, dislike_count]\n",
    "    \n",
    "\n",
    "driver.quit()\n",
    "print(df)\n",
    "\n",
    "#This saves the CSV file.\n",
    "df.to_csv('youtubeVideoData.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c914b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Preparation: This section will describe my procedures for preparing my data.\n",
    "\n",
    "#import the NLTK libraries, including the stopwords that will be removed later\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "#load csv file into pandas dataframe\n",
    "df = pd.read_csv(\"youtubeVideoData.csv\")\n",
    "\n",
    "#convert the string columns into int columns, easier to do it here than when you initially collect it.\n",
    "df['views'] = df['views'].astype('int')\n",
    "df['likes'] = df['likes'].astype('int')\n",
    "df['dislikes'] = df['dislikes'].astype('int')\n",
    "\n",
    "\n",
    "#alter the dataframe to insert two new columns that consist of the ratios of the likes, dislikes, and views\n",
    "df[\"Like/view ratio\"] = df[\"Likes\"] / df[\"Views\"]\n",
    "df[\"Dislike/view ratio\"] = df[\"Dislikes\"] / df[\"Views\"]\n",
    "\n",
    "#Using NLTK's native stopward removal, we define a method to remove all the stopwards, and then apply it to the title column\n",
    "def remove_stopwords(\"text\"):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = nltk.word_tokenize(text)\n",
    "    #these are all remaining words not in the NLTK stopword database\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "#apply remove_stopwards to the title column\n",
    "df[\"Title\"] = df[\"Title\"].apply(remove_stopwords())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ae382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis: This section will describe my procedures for preparing my data\n",
    "#Import NLTK, download the VADER lexicon.\n",
    "import nltk\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "#import the sentiment analyzer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "#initialize\n",
    "sentiment = SentimentIntensityAnalyzer()\n",
    "\n",
    "#add the compound value from the sentiment analysis to the dataframe under \"sentiment value\"\n",
    "#we're using the compound value because it is a single value that expresses the overall positive/negative score rather than multiple\n",
    "#also it only adds one column.\n",
    "df[\"Sentiment Value\"] = df[\"Title\"].apply(sentiment.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "#Now comes correlation analysis. Code adapted from Professor Jeong's code from Google Colab\n",
    "#import seaborns\n",
    "import seaborn as sns\n",
    "#import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#creating the heatmap, defining the color and size of the image produced.\n",
    "dataplot = sns.heatmap(df, cmap=\"YlGnBu\", annot=True)\n",
    "plt.figure(figsize=(30, 15))\n",
    "plt.show()\n",
    "dataplot\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
